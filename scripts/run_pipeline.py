# scripts/run_pipeline.py - ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (config + env + CLI)

import sys
import os
import logging
import argparse
import yaml
from pathlib import Path
from typing import Optional, List, Dict, Any
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.data.pipeline import DataPipeline
from src.data.indicator_calculator import IndicatorCalculator

# ============================================
# ë¡œê¹… ì„¤ì •
# ============================================

def setup_logging(log_level: str = "INFO") -> logging.Logger:
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper(), logging.INFO),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)


# ============================================
# ì„¤ì • ë¡œë” (2-tier cascading)
# ============================================

class ConfigLoader:
    """
    ì„¤ì •ì„ ë‹¤ìŒ ìˆœì„œë¡œ ë¡œë“œí•©ë‹ˆë‹¤:
    1. config.yaml (ê¸°ë³¸ê°’)
    2. CLI ì¸ì (ìµœìš°ì„ )
    """

    def __init__(self, config_file: str = "config.yaml"):
        self.config_file = config_file
        self.logger = logging.getLogger(__name__)
        self.yaml_config = self._load_yaml()

    def _load_yaml(self) -> Dict[str, Any]:
        """YAML ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            if Path(self.config_file).exists():
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    self.logger.info(f"âœ“ YAML config loaded: {self.config_file}")
                    return config or {}
            else:
                self.logger.warning(f"âš  Config file not found: {self.config_file}")
                return {}
        except Exception as e:
            self.logger.error(f"âœ— Failed to load config: {e}")
            return {}

    def get_config(self, cli_args: argparse.Namespace) -> Dict[str, Any]:
        """
        2-tier cascadingìœ¼ë¡œ ìµœì¢… ì„¤ì • ìƒì„±

        ìˆœì„œ:
        1. YAML config (ê¸°ë³¸ê°’)
        2. CLI ì¸ìë¡œ ë®ì–´ì“°ê¸° (ìµœìš°ì„ )
        """
        config = self._build_config_from_yaml()
        config = self._merge_cli_args(config, cli_args)

        self.logger.info(f"\n{'='*70}")
        self.logger.info("Final Configuration")
        self.logger.info(f"{'='*70}")
        self.logger.info(f"Tickers: {config['tickers']}")

        period_str = config.get('period') or f"{config.get('start_date')} ~ {config.get('end_date')}"
        self.logger.info(f"Period: {period_str}")

        self.logger.info(f"Indicators: {config['indicators']}")
        self.logger.info(f"Batch size: {config['batch_size']}")
        self.logger.info(f"Database: {config['database_path']}")
        self.logger.info(f"{'='*70}\n")

        return config

    def _build_config_from_yaml(self) -> Dict[str, Any]:
        """YAML ì„¤ì • íŒŒì¼ì„ ì½ì–´ ì„¤ì • ë¹Œë“œ"""
        config = {
            # ë°ì´í„° ì„¤ì •
            'tickers': self.yaml_config.get('data', {}).get('tickers', ['005930.KS', '000660.KS']),
            'database_path': self.yaml_config.get('data', {}).get('database_path', 'data/database/stocks.db'),
            'period': self.yaml_config.get('data', {}).get('period', '1y'),
            'start_date': self.yaml_config.get('data', {}).get('start_date'),
            'end_date': self.yaml_config.get('data', {}).get('end_date'),
            'update_if_exists': self.yaml_config.get('data', {}).get('update_if_exists', True),
            'interval': self.yaml_config.get('data', {}).get('interval', '1d'),

            # ì§€í‘œ ì„¤ì •
            'indicators': self.yaml_config.get('indicators', {}).get('list', ['ma_5', 'ma_20', 'ma_200', 'macd']),
            'indicator_version': self.yaml_config.get('indicators', {}).get('version', 'v1.0'),
            'lookback_enabled': self.yaml_config.get('indicators', {}).get('lookback', {}).get('enabled', True),

            # ë°°ì¹˜ ì„¤ì •
            'batch_size': self.yaml_config.get('batch', {}).get('size', 100),

            # Fetcher ì„¤ì •
            'max_workers': self.yaml_config.get('fetcher', {}).get('max_workers', 5),
            'max_retries': self.yaml_config.get('fetcher', {}).get('max_retries', 3),

            # ë¡œê¹… ì„¤ì •
            'log_level': self.yaml_config.get('logging', {}).get('level', 'INFO'),
        }

        return config

    def _merge_cli_args(self, config: Dict[str, Any], cli_args: argparse.Namespace) -> Dict[str, Any]:
        """CLI ì¸ìë¡œ config.yaml ì„¤ì • ë®ì–´ì“°ê¸°"""
        # CLIì—ì„œ ì œê³µëœ ì¸ìë“¤ë§Œ ë®ì–´ì“°ê¸°
        if cli_args.tickers:
            config['tickers'] = cli_args.tickers
            self.logger.debug(f"âœ“ Override from CLI: tickers = {config['tickers']}")

        if cli_args.period:
            config['period'] = cli_args.period
            config['start_date'] = None  # ì¶©ëŒ ë°©ì§€
            config['end_date'] = None
            self.logger.debug(f"âœ“ Override from CLI: period = {config['period']}")

        if cli_args.start_date:
            config['start_date'] = cli_args.start_date
            config['period'] = None  # ì¶©ëŒ ë°©ì§€
            self.logger.debug(f"âœ“ Override from CLI: start_date = {config['start_date']}")

        if cli_args.end_date:
            config['end_date'] = cli_args.end_date
            self.logger.debug(f"âœ“ Override from CLI: end_date = {config['end_date']}")

        if cli_args.indicators:
            config['indicators'] = cli_args.indicators
            self.logger.debug(f"âœ“ Override from CLI: indicators = {config['indicators']}")

        if cli_args.batch_size:
            config['batch_size'] = cli_args.batch_size
            self.logger.debug(f"âœ“ Override from CLI: batch_size = {config['batch_size']}")

        if cli_args.update_if_exists is not None:
            config['update_if_exists'] = cli_args.update_if_exists
            self.logger.debug(f"âœ“ Override from CLI: update_if_exists = {config['update_if_exists']}")

        return config


# ============================================
# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ë“¤
# ============================================

def run_full_pipeline(config: Dict[str, Any], logger: logging.Logger) -> None:
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (Step 1-4)"""
    logger.info("\n" + "="*70)
    logger.info("ğŸš€ Full Pipeline ì‹¤í–‰ (Fetch â†’ Save â†’ Calculate â†’ Save Indicators)")
    logger.info("="*70)

    with DataPipeline(
        db_path=config['database_path'],
        max_workers=config['max_workers'],
        max_retries=config['max_retries']
    ) as pipeline:
        results = pipeline.run_full_pipeline(
            ticker_list=config['tickers'],
            start_date=config.get('start_date'),
            end_date=config.get('end_date'),
            period=config.get('period'),
            interval=config['interval'],
            update_if_exists=config['update_if_exists'],
            indicator_list=config['indicators'],
            version=config['indicator_version'],
            batch_size=config['batch_size'],
        )

        _print_results(results, logger)


def run_price_pipeline(config: Dict[str, Any], logger: logging.Logger) -> None:
    """ê°€ê²© ë°ì´í„° íŒŒì´í”„ë¼ì¸ë§Œ (Step 1-2)"""
    logger.info("\n" + "="*70)
    logger.info("ğŸ“Š Price Pipeline ì‹¤í–‰ (Fetch â†’ Save Price Data)")
    logger.info("="*70)

    with DataPipeline(
        db_path=config['database_path'],
        max_workers=config['max_workers'],
        max_retries=config['max_retries']
    ) as pipeline:
        results = pipeline.run_price_pipeline(
            ticker_list=config['tickers'],
            start_date=config.get('start_date'),
            end_date=config.get('end_date'),
            period=config.get('period'),
            interval=config['interval'],
            update_if_exists=config['update_if_exists'],
            batch_size=config['batch_size'],
        )

        _print_results(results, logger)


def run_indicator_pipeline(config: Dict[str, Any], logger: logging.Logger) -> None:
    """ì§€í‘œ íŒŒì´í”„ë¼ì¸ë§Œ (Step 3-4)"""
    logger.info("\n" + "="*70)
    logger.info("ğŸ“ˆ Indicator Pipeline ì‹¤í–‰ (Load â†’ Calculate â†’ Save)")
    logger.info("="*70)

    with DataPipeline(
        db_path=config['database_path'],
        max_workers=config['max_workers'],
        max_retries=config['max_retries']
    ) as pipeline:
        results = pipeline.run_indicator_pipeline(
            ticker_list=config['tickers'],
            indicator_list=config['indicators'],
            start_date=config.get('start_date'),
            end_date=config.get('end_date'),
            version=config['indicator_version'],
            batch_size=config['batch_size'],
        )

        _print_results(results, logger)


def _print_results(results: Any, logger: logging.Logger) -> None:
    """ê²°ê³¼ ì¶œë ¥ (pipeline ë‚´ë¶€ì—ì„œ ì´ë¯¸ ìƒì„¸ ë¡œê¹…ë¨)"""
    # pipeline.run_*() í•¨ìˆ˜ë“¤ì€ DataFrame dictë¥¼ ë°˜í™˜
    # íŒŒì´í”„ë¼ì¸ ë‚´ë¶€ì—ì„œ ì´ë¯¸ ëª¨ë“  ê²°ê³¼ë¥¼ loggerë¡œ ì¶œë ¥í•˜ë¯€ë¡œ
    # ì—¬ê¸°ì„œëŠ” ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€ë§Œ ì¶œë ¥
    logger.info("")


# ============================================
# CLI ì¸ì íŒŒì„œ
# ============================================

def create_parser() -> argparse.ArgumentParser:
    """CLI ì¸ì íŒŒì„œ ìƒì„±"""
    parser = argparse.ArgumentParser(
        description='ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (config.yaml + CLI args)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # config.yaml ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
  python scripts/run_pipeline.py --full

  # CLIë¡œ ì¢…ëª© ì§€ì • (config.yaml ë®ì–´ì“°ê¸°)
  python scripts/run_pipeline.py --full --tickers 005930.KS 000660.KS

  # ê¸°ê°„ ì§€ì • (config.yaml ë®ì–´ì“°ê¸°)
  python scripts/run_pipeline.py --full --period 6m

  # ë‚ ì§œ ë²”ìœ„ ì§€ì •
  python scripts/run_pipeline.py --full --start-date 2024-01-01 --end-date 2024-12-31

  # ê°€ê²© ë°ì´í„°ë§Œ
  python scripts/run_pipeline.py --price

  # ì§€í‘œë§Œ (ê¸°ì¡´ DB ë°ì´í„° ì‚¬ìš©)
  python scripts/run_pipeline.py --indicators

  # ë°°ì¹˜ í¬ê¸° ë³€ê²½
  python scripts/run_pipeline.py --full --batch-size 50

  # ì—¬ëŸ¬ ì˜µì…˜ ì¡°í•©
  python scripts/run_pipeline.py --full --tickers 005930.KS --period 1y --batch-size 50

ì„¤ì • ìš°ì„ ìˆœìœ„:
  config.yaml (ê¸°ë³¸ê°’) â†’ CLI args (ìš°ì„ )
        """
    )

    # íŒŒì´í”„ë¼ì¸ ì„ íƒ
    pipeline_group = parser.add_mutually_exclusive_group(required=True)
    pipeline_group.add_argument(
        '--full',
        action='store_true',
        help='ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (Fetch â†’ Save Price â†’ Calculate â†’ Save Indicators)'
    )
    pipeline_group.add_argument(
        '--price',
        action='store_true',
        help='ê°€ê²© ë°ì´í„° íŒŒì´í”„ë¼ì¸ë§Œ (Fetch â†’ Save Price)'
    )
    pipeline_group.add_argument(
        '--indicators',
        action='store_true',
        help='ì§€í‘œ íŒŒì´í”„ë¼ì¸ë§Œ (Load â†’ Calculate â†’ Save Indicators)'
    )

    # ë°ì´í„° ì„¤ì •
    parser.add_argument(
        '--tickers',
        nargs='+',
        help='ì¢…ëª© ì½”ë“œ (ì˜ˆ: 005930.KS 000660.KS)'
    )
    parser.add_argument(
        '--period',
        help='ê¸°ê°„ (ì˜ˆ: 1y, 6m, 3m, 1m)'
    )
    parser.add_argument(
        '--start-date',
        help='ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD, periodì™€ í•¨ê»˜ ì‚¬ìš© ë¶ˆê°€)'
    )
    parser.add_argument(
        '--end-date',
        help='ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)'
    )

    # ì§€í‘œ ì„¤ì •
    parser.add_argument(
        '--indicators-list',
        dest='indicators',
        nargs='+',
        help='ê³„ì‚°í•  ì§€í‘œ (ì˜ˆ: ma_5 ma_20 ma_200 macd)'
    )

    # ë°°ì¹˜ ì„¤ì •
    parser.add_argument(
        '--batch-size',
        type=int,
        help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 100)'
    )

    # ê¸°íƒ€
    parser.add_argument(
        '--update-if-exists',
        dest='update_if_exists',
        action='store_true',
        help='ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸'
    )
    parser.add_argument(
        '--no-update',
        dest='update_if_exists',
        action='store_false',
        help='ê¸°ì¡´ ë°ì´í„° ìœ ì§€'
    )
    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO',
        help='ë¡œê·¸ ë ˆë²¨ (ê¸°ë³¸: INFO)'
    )
    parser.add_argument(
        '--config',
        default='config.yaml',
        help='ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: config.yaml)'
    )

    return parser


# ============================================
# ë©”ì¸ ì‹¤í–‰
# ============================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = create_parser()
    args = parser.parse_args()

    # ë¡œê¹… ì„¤ì •
    logger = setup_logging(args.log_level)

    try:
        # ì„¤ì • ë¡œë“œ
        config_loader = ConfigLoader(config_file=args.config)
        config = config_loader.get_config(args)

        # íŒŒì´í”„ë¼ì¸ ì„ íƒ ë° ì‹¤í–‰
        if args.full:
            run_full_pipeline(config, logger)
        elif args.price:
            run_price_pipeline(config, logger)
        elif args.indicators:
            run_indicator_pipeline(config, logger)

        logger.info("\nâœ… Pipeline execution completed!")

    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ Pipeline interrupted by user")
        sys.exit(0)
    except Exception as e:
        logger.error(f"\nâŒ Pipeline failed with error: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
