# scripts/run_pipeline.py - íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ìš© CLI (config.yaml + CLI override)
import argparse
import logging
import sys
from pathlib import Path
from typing import Any, Dict

import yaml

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.data.all_ticker import TickerUniverse
from src.data.pipeline import DataPipeline


# ============================================
# ë¡œê¹… ì„¤ì •
# ============================================
def setup_logging(log_level: str = "INFO") -> logging.Logger:
    logging.basicConfig(
        level=getattr(logging, log_level.upper(), logging.INFO),
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )
    return logging.getLogger(__name__)


# ============================================
# ì„¤ì • ë¡œë”
# ============================================
class ConfigLoader:
    """
    config/run_pipeline.yamlì„ ê¸°ë³¸ìœ¼ë¡œ ë¡œë“œí•œë‹¤.
    """

    def __init__(self, config_file: str = "config/run_pipeline.yaml") -> None:
        self.config_file = Path(config_file)
        self.logger = logging.getLogger(__name__)
        self.yaml_config = self._load_yaml()

    def _load_yaml(self) -> Dict[str, Any]:
        if self.config_file.exists():
            with self.config_file.open("r", encoding="utf-8") as f:
                cfg = yaml.safe_load(f)
                self.logger.info("YAML config loaded: %s", self.config_file)
                return cfg or {}
        self.logger.warning("Config file not found: %s", self.config_file)
        return {}

    def get_config(self) -> Dict[str, Any]:
        data_cfg = self.yaml_config.get("data", {}) or {}
        indicators_cfg = self.yaml_config.get("indicators", {}) or {}
        batch_cfg = self.yaml_config.get("batch", {}) or {}
        fetcher_cfg = self.yaml_config.get("fetcher", {}) or {}
        logging_cfg = self.yaml_config.get("logging", {}) or {}

        config =  {
            # ë°ì´í„° ì„¤ì •
            "tickers": data_cfg.get("tickers"),
            "exchanges": data_cfg.get("exchanges"),
            "database_path": data_cfg.get("database_path"),
            "period": data_cfg.get("period", "1y"),
            "start_date": data_cfg.get("start_date"),
            "end_date": data_cfg.get("end_date"),
            "update_if_exists": data_cfg.get("update_if_exists", True),
            "interval": data_cfg.get("interval", "1d"),

            # ì§€í‘œ ì„¤ì •
            "indicators": indicators_cfg.get("list"),
            "indicator_version": indicators_cfg.get("version"),

            # ë°°ì¹˜ ì„¤ì •
            "batch_size": batch_cfg.get("size", 100),

            # Fetcher ì„¤ì •
            "max_workers": fetcher_cfg.get("max_workers", 5),
            "max_retries": fetcher_cfg.get("max_retries", 3),

            # ë¡œê·¸ ì„¤ì •
            "log_level": logging_cfg.get("level", "INFO"),
        }

        #í•„ìˆ˜ ì„¤ì • ê²€ì¦
        essential_fields = ["database_path", "indicators", "indicator_version"]
        missig_fields = [key for key in essential_fields if config[key] is None]
        if missig_fields:
            error_msg = (
                f"\n{'='*40}\n"
                f"ğŸ›‘ ì„¤ì • ì˜¤ë¥˜ (Missing Config)\n"
                f"ë‹¤ìŒ í•„ìˆ˜ í•­ëª©ë“¤ì´ config.yamlì— ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤:\n"
                f" -> {missig_fields}\n"
                f"{'='*40}"
            )
            self.logger.error(error_msg)
            raise ValueError(f"Missing required config fields: {missig_fields}")
        
        #ì„¤ì • ì¶œë ¥
        self.logger.info("\n%s", "=" * 70)
        self.logger.info("Final Configuration")
        self.logger.info("%s", "=" * 70)
        self.logger.info("Tickers: %s", config.get("tickers") or config.get("exchanges"))
        period_str = config.get("period") or f"{config.get('start_date')} ~ {config.get('end_date')}"
        self.logger.info("Period: %s", period_str)
        self.logger.info("Indicators: %s", config["indicators"])
        self.logger.info("Database: %s", config["database_path"])
        self.logger.info("%s\n", "=" * 70)

        return config


# ============================================
# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================
def _resolve_tickers(config: Dict[str, Any]) -> Any:
    """tickersì™€ exchanges ì„¤ì •ì„ í•´ì„í•˜ê³  ìƒì¶© ì‹œ ì—ëŸ¬ë¥¼ ë‚¸ë‹¤."""
    tickers = config.get("tickers") or []
    exchanges = config.get("exchanges") or []

    if tickers and exchanges:
        raise ValueError("tickersì™€ exchangesë¥¼ ë™ì‹œì— ì§€ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•˜ë‚˜ë§Œ ì„ íƒí•˜ì„¸ìš”.")

    if tickers:
        return tickers
    if exchanges:
        return TickerUniverse().get(exchanges)
    raise ValueError("í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. tickers ë˜ëŠ” exchangesë¥¼ ì§€ì •í•˜ì„¸ìš”.")


def run_full_pipeline(config: Dict[str, Any], logger: logging.Logger) -> None:
    logger.info("\n" + "=" * 70)
    logger.info("â–¶ Full Pipeline ì‹¤í–‰ (Fetch â†’ Save â†’ Calculate â†’ Save Indicators)")
    logger.info("=" * 70)

    with DataPipeline(
        db_path=config["database_path"],
        max_workers=config["max_workers"],
        max_retries=config["max_retries"],
        batch_size_default=config["batch_size"],
    ) as pipeline:
        results = pipeline.run_full_pipeline(
            ## ì„ì‹œìˆ˜ì •!!
            ticker_list=_resolve_tickers(config)[:20],
            start_date=config.get("start_date"),
            end_date=config.get("end_date"),
            period=config.get("period"),
            interval=config["interval"],
            update_if_exists=config["update_if_exists"],
            indicator_list=config["indicators"],
            version=config["indicator_version"],
            batch_size=config["batch_size"],
        )
        _print_results(results, logger)


def run_price_pipeline(config: Dict[str, Any], logger: logging.Logger) -> None:
    logger.info("\n" + "=" * 70)
    logger.info("â–¶ Price Pipeline ì‹¤í–‰ (Fetch â†’ Save Price Data)")
    logger.info("=" * 70)

    with DataPipeline(
        db_path=config["database_path"],
        max_workers=config["max_workers"],
        max_retries=config["max_retries"],
        batch_size_default=config["batch_size"],
    ) as pipeline:
        results = pipeline.run_price_pipeline(
            ticker_list=_resolve_tickers(config),
            start_date=config.get("start_date"),
            end_date=config.get("end_date"),
            period=config.get("period"),
            interval=config["interval"],
            update_if_exists=config["update_if_exists"],
            batch_size=config["batch_size"],
        )
        _print_results(results, logger)


def run_indicator_pipeline(config: Dict[str, Any], logger: logging.Logger) -> None:
    logger.info("\n" + "=" * 70)
    logger.info("â–¶ Indicator Pipeline ì‹¤í–‰ (Load â†’ Calculate â†’ Save)")
    logger.info("=" * 70)

    with DataPipeline(
        db_path=config["database_path"],
        max_workers=config["max_workers"],
        max_retries=config["max_retries"],
        batch_size_default=config["batch_size"],
    ) as pipeline:
        results = pipeline.run_indicator_pipeline(
            ticker_list=_resolve_tickers(config),
            indicator_list=config["indicators"],
            start_date=config.get("start_date"),
            end_date=config.get("end_date"),
            version=config["indicator_version"],
            batch_size=config["batch_size"],
        )
        _print_results(results, logger)


def _print_results(results: Any, logger: logging.Logger) -> None:
    logger.info("Result summary: %s", results.get("summary"))


# ============================================
# CLI íŒŒì„œ
# ============================================
def create_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ê¸°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    pipeline_group = parser.add_mutually_exclusive_group(required=True)
    pipeline_group.add_argument("--full", action="store_true", help="ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
    pipeline_group.add_argument("--price", action="store_true", help="ê°€ê²© íŒŒì´í”„ë¼ì¸ë§Œ ì‹¤í–‰")
    pipeline_group.add_argument("--indicators", action="store_true", help="ì§€í‘œ íŒŒì´í”„ë¼ì¸ë§Œ ì‹¤í–‰")

    return parser


# ============================================
# ë©”ì¸ ì‹¤í–‰
# ============================================
def main():
    parser = create_parser()
    args = parser.parse_args()
    logger = setup_logging("INFO")

    try:
        config_loader = ConfigLoader()
        config = config_loader.get_config()

        new_level = config.get("log_level", "INFO").upper()
        logging.getLogger().setLevel(getattr(logging, new_level))

        if args.full:
            run_full_pipeline(config, logger)
        elif args.price:
            run_price_pipeline(config, logger)
        elif args.indicators:
            run_indicator_pipeline(config, logger)

        logger.info("\nâœ… Pipeline execution completed!")

    except KeyboardInterrupt:
        logger.warning("\nâ›” Pipeline interrupted by user")
        sys.exit(0)
    except Exception as exc:
        logger.error("\nâŒ Pipeline failed with error: %s", exc, exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
